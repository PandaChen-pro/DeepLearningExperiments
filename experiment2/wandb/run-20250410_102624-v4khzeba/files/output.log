Files already downloaded and verified
Files already downloaded and verified
/data/coding/deep_learning_experiments/experiment2/start.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path, map_location=device)
Epoch 1/50: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [08:19<00:00,  1.28s/it]
Epoch 1, Batch 0, Loss: 0.0028, learning_rate: 0.000001
Epoch 1, Batch 30, Loss: 0.0001, learning_rate: 0.000003
Epoch 1, Batch 60, Loss: 0.0000, learning_rate: 0.000004
Epoch 1, Batch 90, Loss: 0.0001, learning_rate: 0.000006
Epoch 1, Batch 120, Loss: 0.0000, learning_rate: 0.000007
Epoch 1, Batch 150, Loss: 0.0003, learning_rate: 0.000009
Epoch 1, Batch 180, Loss: 0.0001, learning_rate: 0.000010
Epoch 1, Batch 210, Loss: 0.0002, learning_rate: 0.000012
Epoch 1, Batch 240, Loss: 0.0001, learning_rate: 0.000013
Epoch 1, Batch 270, Loss: 0.0001, learning_rate: 0.000015
Epoch 1, Batch 300, Loss: 0.0005, learning_rate: 0.000016
Epoch 1, Batch 330, Loss: 0.0370, learning_rate: 0.000018
Epoch 1, Batch 360, Loss: 0.0301, learning_rate: 0.000019
Epoch 1, Batch 390, Loss: 0.0005, learning_rate: 0.000021
Epoch [1/50] - Loss: 0.0019, Accuracy: 99.95%
starting validation......
Validation Loss: 1.9507, Validation Accuracy: 73.47%
Epoch 1/50, Val Loss: 1.9507, Val Acc: 73.47%
save model to ./checkpoints/ViT_Original_Val_Epoch1_Acc73.47.pth
Epoch 2/50: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [08:20<00:00,  1.28s/it]
Epoch 2, Batch 0, Loss: 0.0005, learning_rate: 0.000021
Epoch 2, Batch 30, Loss: 0.0014, learning_rate: 0.000022
Epoch 2, Batch 60, Loss: 0.0130, learning_rate: 0.000024
Epoch 2, Batch 90, Loss: 0.0363, learning_rate: 0.000025
Epoch 2, Batch 120, Loss: 0.0004, learning_rate: 0.000027
Epoch 2, Batch 150, Loss: 0.0006, learning_rate: 0.000028
Epoch 2, Batch 180, Loss: 0.0021, learning_rate: 0.000030
Epoch 2, Batch 210, Loss: 0.0071, learning_rate: 0.000031
Epoch 2, Batch 240, Loss: 0.0040, learning_rate: 0.000033
Epoch 2, Batch 270, Loss: 0.0289, learning_rate: 0.000034
Epoch 2, Batch 300, Loss: 0.0157, learning_rate: 0.000036
Epoch 2, Batch 330, Loss: 0.0715, learning_rate: 0.000038
Epoch 2, Batch 360, Loss: 0.0483, learning_rate: 0.000039
Epoch 2, Batch 390, Loss: 0.0152, learning_rate: 0.000041
Epoch [2/50] - Loss: 0.0131, Accuracy: 99.57%
Epoch 3/50: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [08:19<00:00,  1.28s/it]
Epoch 3, Batch 0, Loss: 0.0031, learning_rate: 0.000041
Epoch 3, Batch 30, Loss: 0.0027, learning_rate: 0.000042
Epoch 3, Batch 60, Loss: 0.0181, learning_rate: 0.000044
Epoch 3, Batch 90, Loss: 0.0006, learning_rate: 0.000045
Epoch 3, Batch 120, Loss: 0.0247, learning_rate: 0.000047
Epoch 3, Batch 150, Loss: 0.0340, learning_rate: 0.000048
Epoch 3, Batch 180, Loss: 0.0376, learning_rate: 0.000050
Epoch 3, Batch 210, Loss: 0.0070, learning_rate: 0.000051
Epoch 3, Batch 240, Loss: 0.0160, learning_rate: 0.000053
Epoch 3, Batch 270, Loss: 0.0037, learning_rate: 0.000054
Epoch 3, Batch 300, Loss: 0.0050, learning_rate: 0.000056
Epoch 3, Batch 330, Loss: 0.0181, learning_rate: 0.000057
Epoch 3, Batch 360, Loss: 0.0348, learning_rate: 0.000059
Epoch 3, Batch 390, Loss: 0.0818, learning_rate: 0.000060
Epoch [3/50] - Loss: 0.0273, Accuracy: 99.08%
starting validation......
Validation Loss: 1.6656, Validation Accuracy: 71.79%
Epoch 3/50, Val Loss: 1.6656, Val Acc: 71.79%
Epoch 4/50:  54%|██████████████████████████████████████████████████████████████████████████████▊                                                                   | 211/391 [04:32<03:52,  1.29s/it]
Epoch 4, Batch 0, Loss: 0.0333, learning_rate: 0.000060
Epoch 4, Batch 30, Loss: 0.0976, learning_rate: 0.000062
Epoch 4, Batch 60, Loss: 0.0435, learning_rate: 0.000063
Epoch 4, Batch 90, Loss: 0.0072, learning_rate: 0.000065
Epoch 4, Batch 120, Loss: 0.0392, learning_rate: 0.000066
Epoch 4, Batch 150, Loss: 0.0260, learning_rate: 0.000068
Epoch 4, Batch 180, Loss: 0.1079, learning_rate: 0.000070
Epoch 4, Batch 210, Loss: 0.0785, learning_rate: 0.000071
Traceback (most recent call last):
  File "/data/coding/deep_learning_experiments/experiment2/start.py", line 83, in <module>
  File "/data/coding/deep_learning_experiments/experiment2/trainer.py", line 40, in train_model
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7f2fc60f5ab0>
Traceback (most recent call last):
  File "/data/miniconda/envs/cv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/data/miniconda/envs/cv/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/data/miniconda/envs/cv/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/data/miniconda/envs/cv/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/data/miniconda/envs/cv/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
